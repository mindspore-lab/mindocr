
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://mindspore-lab.github.io/mindocr/mkdocs/online_inference/">
      
      
        <link rel="prev" href="../../tutorials/advanced_train/">
      
      
        <link rel="next" href="../../inference/inference_tutorial/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.9">
    
    
      
        <title>Python Online Inference - MindOCR Docs</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.4af4bdda.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#mindocr-online-inference" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="MindOCR Docs" class="md-header__button md-logo" aria-label="MindOCR Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            MindOCR Docs
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Python Online Inference
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
      <div class="md-header__option">
  <div class="md-select">
    
    <button class="md-header__button md-icon" aria-label="Select language">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m12.87 15.07-2.54-2.51.03-.03A17.5 17.5 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2zm-2.62 7 1.62-4.33L19.12 17z"/></svg>
    </button>
    <div class="md-select__inner">
      <ul class="md-select__list">
        
          <li class="md-select__item">
            <a href="./" hreflang="en" class="md-select__link">
              English
            </a>
          </li>
        
          <li class="md-select__item">
            <a href="../../zh/mkdocs/online_inference/" hreflang="zh" class="md-select__link">
              中文
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</div>
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/mindspore-lab/mindocr" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    mindspore-lab/mindocr
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../modelzoo_training/" class="md-tabs__link">
          
  
    
  
  Model Zoo

        </a>
      </li>
    
  

      
        
  
  
    
  
  
    
    
      
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../datasets/converters/" class="md-tabs__link">
          
  
    
  
  Tutorials

        </a>
      </li>
    
  

    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../contributing/" class="md-tabs__link">
          
  
    
  
  Notes

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="MindOCR Docs" class="md-nav__button md-logo" aria-label="MindOCR Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    MindOCR Docs
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/mindspore-lab/mindocr" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    mindspore-lab/mindocr
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Model Zoo
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Model Zoo
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modelzoo_training/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Training
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../inference/mindocr_models_list/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Inference - MindOCR Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../inference/thirdparty_models_list/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Inference - Third-party Models
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Tutorials
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Tutorials
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="">
            
  
  <span class="md-ellipsis">
    1. Datasets
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            1. Datasets
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../datasets/converters/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Dataset Preparation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tutorials/transform_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data Transformation Mechanism
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    2. Model Training
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            2. Model Training
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tutorials/yaml_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Yaml Configuration
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tutorials/training_detection_custom_dataset/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Text Detection
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tutorials/training_recognition_custom_dataset/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Text Recognition
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tutorials/distribute_train/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Distributed Training
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tutorials/advanced_train/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Advance Training
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3_3" id="__nav_3_3_label" tabindex="">
            
  
  <span class="md-ellipsis">
    3. Inference and Deployment
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3_3">
            <span class="md-nav__icon md-icon"></span>
            3. Inference and Deployment
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Python Online Inference
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Python Online Inference
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#dependency-and-installation" class="md-nav__link">
    <span class="md-ellipsis">
      Dependency and Installation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#text-detection" class="md-nav__link">
    <span class="md-ellipsis">
      Text Detection
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Text Detection">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#supported-detection-algorithms-and-networks" class="md-nav__link">
    <span class="md-ellipsis">
      Supported Detection Algorithms and Networks
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#text-recognition" class="md-nav__link">
    <span class="md-ellipsis">
      Text Recognition
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Text Recognition">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#supported-recognition-algorithms-and-networks" class="md-nav__link">
    <span class="md-ellipsis">
      Supported Recognition Algorithms and Networks
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#text-detection-and-recognition-concatenation" class="md-nav__link">
    <span class="md-ellipsis">
      Text Detection and Recognition Concatenation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Text Detection and Recognition Concatenation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#supported-detection-algorithms-and-networks_1" class="md-nav__link">
    <span class="md-ellipsis">
      Supported Detection Algorithms and Networks
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#evaluation-of-the-inference-results" class="md-nav__link">
    <span class="md-ellipsis">
      Evaluation of the Inference Results
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#text-direction-classification" class="md-nav__link">
    <span class="md-ellipsis">
      Text direction classification
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#table-structure-recognition" class="md-nav__link">
    <span class="md-ellipsis">
      Table Structure Recognition
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Table Structure Recognition">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#supported-table-structure-recognition-algorithms-and-networks" class="md-nav__link">
    <span class="md-ellipsis">
      Supported Table Structure Recognition Algorithms and Networks
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#table-structure-recognition-and-text-detection-recognition-concatenation" class="md-nav__link">
    <span class="md-ellipsis">
      Table Structure Recognition and Text Detection Recognition Concatenation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#layout-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      Layout Analysis
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#end-to-end-document-analysis-and-recovery" class="md-nav__link">
    <span class="md-ellipsis">
      End-to-end Document Analysis and Recovery
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#argument-list" class="md-nav__link">
    <span class="md-ellipsis">
      Argument List
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#developer-guide-how-to-add-a-new-model-for-inference" class="md-nav__link">
    <span class="md-ellipsis">
      Developer Guide - How to Add a New Model for Inference
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Developer Guide - How to Add a New Model for Inference">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#preprocessing" class="md-nav__link">
    <span class="md-ellipsis">
      Preprocessing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#network-inference" class="md-nav__link">
    <span class="md-ellipsis">
      Network Inference
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#postproprocess" class="md-nav__link">
    <span class="md-ellipsis">
      Postproprocess
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../inference/inference_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    MindOCR Offline Inference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../inference/mindocr_models_list/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Inference - MindOCR Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../inference/thirdparty_models_list/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Inference - Third-party Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../inference/convert_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Model Conversion
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4" >
        
          
          <label class="md-nav__link" for="__nav_3_4" id="__nav_3_4_label" tabindex="">
            
  
  <span class="md-ellipsis">
    4. Developer Guides
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_4">
            <span class="md-nav__icon md-icon"></span>
            4. Developer Guides
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../customize_dataset/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Customize Dataset
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../customize_data_transform/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Customize Data Transformation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../customize_model/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Customize a New Model
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../customize_postprocess/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Customize Postprocessing Method
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Notes
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Notes
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../contributing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Contributing
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tutorials/frequently_asked_questions/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    FAQ
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#dependency-and-installation" class="md-nav__link">
    <span class="md-ellipsis">
      Dependency and Installation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#text-detection" class="md-nav__link">
    <span class="md-ellipsis">
      Text Detection
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Text Detection">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#supported-detection-algorithms-and-networks" class="md-nav__link">
    <span class="md-ellipsis">
      Supported Detection Algorithms and Networks
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#text-recognition" class="md-nav__link">
    <span class="md-ellipsis">
      Text Recognition
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Text Recognition">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#supported-recognition-algorithms-and-networks" class="md-nav__link">
    <span class="md-ellipsis">
      Supported Recognition Algorithms and Networks
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#text-detection-and-recognition-concatenation" class="md-nav__link">
    <span class="md-ellipsis">
      Text Detection and Recognition Concatenation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Text Detection and Recognition Concatenation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#supported-detection-algorithms-and-networks_1" class="md-nav__link">
    <span class="md-ellipsis">
      Supported Detection Algorithms and Networks
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#evaluation-of-the-inference-results" class="md-nav__link">
    <span class="md-ellipsis">
      Evaluation of the Inference Results
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#text-direction-classification" class="md-nav__link">
    <span class="md-ellipsis">
      Text direction classification
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#table-structure-recognition" class="md-nav__link">
    <span class="md-ellipsis">
      Table Structure Recognition
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Table Structure Recognition">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#supported-table-structure-recognition-algorithms-and-networks" class="md-nav__link">
    <span class="md-ellipsis">
      Supported Table Structure Recognition Algorithms and Networks
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#table-structure-recognition-and-text-detection-recognition-concatenation" class="md-nav__link">
    <span class="md-ellipsis">
      Table Structure Recognition and Text Detection Recognition Concatenation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#layout-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      Layout Analysis
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#end-to-end-document-analysis-and-recovery" class="md-nav__link">
    <span class="md-ellipsis">
      End-to-end Document Analysis and Recovery
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#argument-list" class="md-nav__link">
    <span class="md-ellipsis">
      Argument List
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#developer-guide-how-to-add-a-new-model-for-inference" class="md-nav__link">
    <span class="md-ellipsis">
      Developer Guide - How to Add a New Model for Inference
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Developer Guide - How to Add a New Model for Inference">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#preprocessing" class="md-nav__link">
    <span class="md-ellipsis">
      Preprocessing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#network-inference" class="md-nav__link">
    <span class="md-ellipsis">
      Network Inference
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#postproprocess" class="md-nav__link">
    <span class="md-ellipsis">
      Postproprocess
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/mindspore-lab/mindocr/edit/master/docs/en/mkdocs/online_inference.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/mindspore-lab/mindocr/raw/master/docs/en/mkdocs/online_inference.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg>
    </a>
  


<!-- BEGIN INCLUDE ../../../tools/infer/text/README.md '&lt;!--start--&gt;' '&lt;!--end--&gt;' -->
<h1 id="mindocr-online-inference">MindOCR Online Inference<a class="headerlink" href="#mindocr-online-inference" title="Permanent link">&para;</a></h1>
<p><strong>About Online Inference:</strong> Online inference is to infer based on the native MindSpore framework by loading the model checkpoint file then running prediction with MindSpore APIs.</p>
<p>Compared to offline inference (which is implemented in <code>deploy/py_infer</code> in MindOCR), online inferece does not require model conversion for target platforms and can run directly on the training devices (e.g. Ascend 910). But it requires installing the heavy AI framework and the model is not optimized for deployment.</p>
<p>Thus, online inference is more suitable for demonstration and to visually evaluate model generalization ability on unseen data.</p>
<h2 id="dependency-and-installation">Dependency and Installation<a class="headerlink" href="#dependency-and-installation" title="Permanent link">&para;</a></h2>
<p>To be consistent with training environment.</p>
<h2 id="text-detection">Text Detection<a class="headerlink" href="#text-detection" title="Permanent link">&para;</a></h2>
<p>To run text detection on an input image or a directory containing multiple images, please execute</p>
<div class="highlight"><pre><span></span><code>python<span class="w"> </span>tools/infer/text/predict_det.py<span class="w">  </span>--image_dir<span class="w"> </span><span class="o">{</span>path_to_img<span class="w"> </span>or<span class="w"> </span>dir_to_imgs<span class="o">}</span><span class="w"> </span>--det_algorithm<span class="w"> </span>DB++
</code></pre></div>
<p>After running, the inference results will be saved in <code>{args.draw_img_save_dir}/det_results.txt</code>, where <code>--draw_img_save_dir</code> is the directory for saving  results and is set to <code>./inference_results</code> by default Here are some results for examples.</p>
<p>Example 1:</p>
<p align="center">
  <img src="https://github.com/SamitHuang/mindocr-1/assets/8156835/ce136b92-f0aa-4a05-b689-9f60d0b40db1" width=480 />
</p>
<p align="center">
  <em> Visualization of text detection result on img_108.jpg</em>
</p>

<p>, where the saved txt file is as follows
<div class="highlight"><pre><span></span><code>img_108.jpg [[[228, 440], [403, 413], [406, 433], [231, 459]], [[282, 280], [493, 252], [499, 293], [288, 321]], [[500, 253], [636, 232], [641, 269], [505, 289]], ...]
</code></pre></div></p>
<p>Example 2:</p>
<p align="center">
  <img src="https://github.com/SamitHuang/mindocr-1/assets/8156835/61066d4a-5922-471e-b702-2ea79c3cc525" width=480 />
</p>
<p align="center">
  <em>Visualization of text detection result on paper_sam.png</em>
</p>

<p>, where the saved txt file is as follows
<div class="highlight"><pre><span></span><code>paper_sam.png   [[[1161, 340], [1277, 340], [1277, 378], [1161, 378]], [[895, 335], [1152, 340], [1152, 382], [894, 378]], ...]
</code></pre></div></p>
<p><strong>Notes:</strong>
- For input images with high resolution, please set <code>--det_limit_side_len</code> larger, e.g., 1280. <code>--det_limit_type</code> can be set as "min" or "max", where "min " means limiting the image size to be at least  <code>--det_limit_side_len</code>, "max" means limiting the image size to be at most <code>--det_limit_side_len</code>.</p>
<ul>
<li>
<p>For more argument illustrations and usage, please run <code>python tools/infer/text/predict_det.py -h</code> or view <code>tools/infer/text/config.py</code></p>
</li>
<li>
<p>Currently, this script runs serially to avoid dynamic shape issue and achieve better performance.</p>
</li>
</ul>
<h3 id="supported-detection-algorithms-and-networks">Supported Detection Algorithms and Networks<a class="headerlink" href="#supported-detection-algorithms-and-networks" title="Permanent link">&para;</a></h3>
<p><center></p>
<table>
<thead>
<tr>
<th style="text-align: center;"><strong>Algorithm Name</strong></th>
<th style="text-align: center;"><strong>Network Name</strong></th>
<th style="text-align: center;"><strong>Language</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">DB</td>
<td style="text-align: center;">dbnet_resnet50</td>
<td style="text-align: center;">English</td>
</tr>
<tr>
<td style="text-align: center;">DB++</td>
<td style="text-align: center;">dbnetpp_resnet50</td>
<td style="text-align: center;">English</td>
</tr>
<tr>
<td style="text-align: center;">DB_MV3</td>
<td style="text-align: center;">dbnet_mobilenetv3</td>
<td style="text-align: center;">English</td>
</tr>
<tr>
<td style="text-align: center;">PSE</td>
<td style="text-align: center;">psenet_resnet152</td>
<td style="text-align: center;">English</td>
</tr>
</tbody>
</table>
<p></center></p>
<p>The algorithm-network mapping is defined in <code>tools/infer/text/predict_det.py</code>.</p>
<h2 id="text-recognition">Text Recognition<a class="headerlink" href="#text-recognition" title="Permanent link">&para;</a></h2>
<p>To run text recognition on an input image or a directory containing multiple images, please execute</p>
<p><div class="highlight"><pre><span></span><code>python<span class="w"> </span>tools/infer/text/predict_rec.py<span class="w">  </span>--image_dir<span class="w"> </span><span class="o">{</span>path_to_img<span class="w"> </span>or<span class="w"> </span>dir_to_imgs<span class="o">}</span><span class="w"> </span>--rec_algorithm<span class="w"> </span>CRNN
</code></pre></div>
After running, the inference results will be saved in <code>{args.draw_img_save_dir}/rec_results.txt</code>, where <code>--draw_img_save_dir</code> is the directory for saving  results and is set to <code>./inference_results</code> by default. Here are some results for examples.</p>
<ul>
<li>English text recognition</li>
</ul>
<p align="center">
  <img src="https://github.com/SamitHuang/mindocr-1/assets/8156835/fa8c5e4e-0e05-4c93-b9a3-6e0327c1609f" width=150 />
</p>
<p align="center">
  <em> word_1216.png </em>
</p>

<p align="center">
  <img src="https://github.com/SamitHuang/mindocr-1/assets/8156835/8ec50bdf-ea6c-4bce-a799-2fdb8e9512b1" width=150 />
</p>
<p align="center">
  <em> word_1217.png </em>
</p>

<p>Recognition results:
<div class="highlight"><pre><span></span><code>word_1216.png   coffee
word_1217.png   club
</code></pre></div></p>
<ul>
<li>Chinese text recognition:</li>
</ul>
<p align="center">
  <img src="https://github.com/SamitHuang/mindocr-1/assets/8156835/e220ade5-89ae-47a4-927f-2c28941a5965" width=200 />
</p>
<p align="center">
  <em> cert_id.png </em>
</p>

<p align="center">
  <img src="https://github.com/SamitHuang/mindocr-1/assets/8156835/d7cfee90-d586-4796-9ebf-b56872832e71" width=400 />
</p>
<p align="center">
  <em> doc_cn3.png </em>
</p>

<p>Recognition results:
<div class="highlight"><pre><span></span><code>cert_id.png 公民身份号码44052419
doc_cn3.png 马拉松选手不会为短暂的领先感到满意，而是永远在奔跑。
</code></pre></div></p>
<p><strong>Notes:</strong>
- For more argument illustrations and usage, please run <code>python tools/infer/text/predict_rec.py -h</code> or view <code>tools/infer/text/config.py</code>
- Both batch-wise and single-mode inference are supported. Batch mode is enabled by default for better speed. You can set the batch size via <code>--rec_batch_size</code>. You can also run in single-mode by set <code>--det_batch_mode</code> False, which may improve accuracy if the text length varies a lot.</p>
<h3 id="supported-recognition-algorithms-and-networks">Supported Recognition Algorithms and Networks<a class="headerlink" href="#supported-recognition-algorithms-and-networks" title="Permanent link">&para;</a></h3>
<p><center></p>
<table>
<thead>
<tr>
<th style="text-align: center;"><strong>Algorithm Name</strong></th>
<th style="text-align: center;"><strong>Network Name</strong></th>
<th style="text-align: center;"><strong>Language</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CRNN</td>
<td style="text-align: center;">crnn_resnet34</td>
<td style="text-align: center;">English</td>
</tr>
<tr>
<td style="text-align: center;">RARE</td>
<td style="text-align: center;">rare_resnet34</td>
<td style="text-align: center;">English</td>
</tr>
<tr>
<td style="text-align: center;">SVTR</td>
<td style="text-align: center;">svtr_tiny</td>
<td style="text-align: center;">English</td>
</tr>
<tr>
<td style="text-align: center;">CRNN_CH</td>
<td style="text-align: center;">crnn_resnet34_ch</td>
<td style="text-align: center;">Chinese</td>
</tr>
<tr>
<td style="text-align: center;">RARE_CH</td>
<td style="text-align: center;">rare_resnet34_ch</td>
<td style="text-align: center;">Chinese</td>
</tr>
</tbody>
</table>
<p></center></p>
<p>The algorithm-network mapping is defined in <code>tools/infer/text/predict_rec.py</code></p>
<p>Currently, space char recognition is not supported for the listed models. We will support it soon.</p>
<h2 id="text-detection-and-recognition-concatenation">Text Detection and Recognition Concatenation<a class="headerlink" href="#text-detection-and-recognition-concatenation" title="Permanent link">&para;</a></h2>
<p>To run text spoting (i.e., detect all text regions then recognize each of them) on an input image or multiple images in a directory, please run:</p>
<div class="highlight"><pre><span></span><code>python<span class="w"> </span>tools/infer/text/predict_system.py<span class="w"> </span>--image_dir<span class="w"> </span><span class="o">{</span>path_to_img<span class="w"> </span>or<span class="w"> </span>dir_to_imgs<span class="o">}</span><span class="w"> </span><span class="se">\</span>
<span class="w">                                          </span>--det_algorithm<span class="w"> </span>DB++<span class="w">  </span><span class="se">\</span>
<span class="w">                                          </span>--rec_algorithm<span class="w"> </span>CRNN
</code></pre></div>
<blockquote>
<p>Note: set <code>--visualize_output True</code> if you want to visualize the detection and recognition results on the input image.</p>
</blockquote>
<p>After running, the inference results will be saved in <code>{args.draw_img_save_dir}/system_results.txt</code>,  where <code>--draw_img_save_dir</code> is the directory for saving  results and is set to <code>./inference_results</code> by default. Here are some results for examples.</p>
<p>Example 1:</p>
<p align="center">
  <img src="https://github.com/SamitHuang/mindocr-1/assets/8156835/c1f53970-8618-4039-994f-9f6dc1eee1dd" width=600 />
</p>
<p align="center">
  <em> Visualization of text detection and recognition result on img_10.jpg </em>
</p>

<p>, where the saved txt file is as follows
<div class="highlight"><pre><span></span><code>img_10.jpg  [{&quot;transcription&quot;: &quot;residential&quot;, &quot;points&quot;: [[43, 88], [149, 78], [151, 101], [44, 111]]}, {&quot;transcription&quot;: &quot;areas&quot;, &quot;points&quot;: [[152, 83], [201, 81], [202, 98], [153, 100]]}, {&quot;transcription&quot;: &quot;when&quot;, &quot;points&quot;: [[36, 56], [101, 56], [101, 78], [36, 78]]}, {&quot;transcription&quot;: &quot;you&quot;, &quot;points&quot;: [[99, 54], [143, 52], [144, 78], [100, 80]]}, {&quot;transcription&quot;: &quot;pass&quot;, &quot;points&quot;: [[140, 54], [186, 50], [188, 74], [142, 78]]}, {&quot;transcription&quot;: &quot;by&quot;, &quot;points&quot;: [[182, 52], [208, 52], [208, 75], [182, 75]]}, {&quot;transcription&quot;: &quot;volume&quot;, &quot;points&quot;: [[199, 30], [254, 30], [254, 46], [199, 46]]}, {&quot;transcription&quot;: &quot;your&quot;, &quot;points&quot;: [[164, 28], [203, 28], [203, 46], [164, 46]]}, {&quot;transcription&quot;: &quot;lower&quot;, &quot;points&quot;: [[109, 25], [162, 25], [162, 46], [109, 46]]}, {&quot;transcription&quot;: &quot;please&quot;, &quot;points&quot;: [[31, 18], [109, 20], [108, 48], [30, 46]]}]
</code></pre></div></p>
<p>Example 2:</p>
<p align="center">
  <img src="https://github.com/SamitHuang/mindocr-1/assets/8156835/c58fb182-32b0-4b73-b4fd-7ba393e3f397" width=480 />
</p>
<p align="center">
  <em> Visualization of text detection and recognition result on web_cvpr.png </em>
</p>

<p>, where the saved txt file is as follows</p>
<div class="highlight"><pre><span></span><code>web_cvpr.png    [{&quot;transcription&quot;: &quot;canada&quot;, &quot;points&quot;: [[430, 148], [540, 148], [540, 171], [430, 171]]}, {&quot;transcription&quot;: &quot;vancouver&quot;, &quot;points&quot;: [[263, 148], [420, 148], [420, 171], [263, 171]]}, {&quot;transcription&quot;: &quot;cvpr&quot;, &quot;points&quot;: [[32, 69], [251, 63], [254, 174], [35, 180]]}, {&quot;transcription&quot;: &quot;2023&quot;, &quot;points&quot;: [[194, 44], [256, 45], [255, 72], [194, 70]]}, {&quot;transcription&quot;: &quot;june&quot;, &quot;points&quot;: [[36, 45], [110, 44], [110, 70], [37, 71]]}, {&quot;transcription&quot;: &quot;1822&quot;, &quot;points&quot;: [[114, 43], [190, 45], [190, 70], [113, 69]]}]
</code></pre></div>
<p><strong>Notes:</strong>
1. For more argument illustrations and usage, please run <code>python tools/infer/text/predict_system.py -h</code> or view <code>tools/infer/text/config.py</code></p>
<h3 id="supported-detection-algorithms-and-networks_1">Supported Detection Algorithms and Networks<a class="headerlink" href="#supported-detection-algorithms-and-networks_1" title="Permanent link">&para;</a></h3>
<p><center></p>
<table>
<thead>
<tr>
<th style="text-align: center;"><strong>Algorithm Name</strong></th>
<th style="text-align: center;"><strong>Network Name</strong></th>
<th style="text-align: center;"><strong>Language</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">YOLOv8</td>
<td style="text-align: center;">yolov8</td>
<td style="text-align: center;">English</td>
</tr>
</tbody>
</table>
<p></center></p>
<p>The algorithm-network mapping is defined in <code>tools/infer/text/predict_layout.py</code>.</p>
<h3 id="evaluation-of-the-inference-results">Evaluation of the Inference Results<a class="headerlink" href="#evaluation-of-the-inference-results" title="Permanent link">&para;</a></h3>
<p>To infer on the whole <a href="https://rrc.cvc.uab.es/?ch=4&amp;com=downloads">ICDAR15</a> test set, please run:
<div class="highlight"><pre><span></span><code>python tools/infer/text/predict_system.py --image_dir /path/to/icdar15/det/test_images  /
                                          --det_algorithm {DET_ALGO}    /
                                          --rec_algorithm {REC_ALGO}  /
                                          --det_limit_type min  /
                                          --det_limit_side_len 720
</code></pre></div></p>
<blockquote>
<p>Note: Here we set<code>det_limit_type</code> as <code>min</code> for better performance, due to the input image in ICDAR15 is of high resolution (720x1280).</p>
</blockquote>
<p>After running, the results including image names, bounding boxes (<code>points</code>) and recognized texts (<code>transcription</code>) will be saved in <code>{args.draw_img_save_dir}/system_results.txt</code>. The format of prediction results is shown as follows.</p>
<div class="highlight"><pre><span></span><code>img_1.jpg   [{&quot;transcription&quot;: &quot;hello&quot;, &quot;points&quot;: [600, 150, 715, 157, 714, 177, 599, 170]}, {&quot;transcription&quot;: &quot;world&quot;, &quot;points&quot;: [622, 126, 695, 129, 694, 154, 621, 151]}, ...]
img_2.jpg   [{&quot;transcription&quot;: &quot;apple&quot;, &quot;points&quot;: [553, 338, 706, 318, 709, 342, 556, 362]}, ...]
   ...
</code></pre></div>
<p>Prepare the <strong>ground truth</strong> file (in the same format as above), which can be obtained from the dataset conversion script in <code>tools/dataset_converters</code>, and run the following command to evaluate the prediction results.</p>
<div class="highlight"><pre><span></span><code>python<span class="w"> </span>deploy/eval_utils/eval_pipeline.py<span class="w"> </span>--gt_path<span class="w"> </span>path/to/gt.txt<span class="w"> </span>--pred_path<span class="w"> </span>path/to/system_results.txt
</code></pre></div>
<p>Evaluation of the text spotting inference results on Ascend 910 with MindSpore 2.0rc1 are shown as follows.</p>
<p><center></p>
<table>
<thead>
<tr>
<th>Det. Algorithm</th>
<th>Rec. Algorithm</th>
<th>Dataset</th>
<th>Accuracy(%)</th>
<th>FPS (imgs/s)</th>
</tr>
</thead>
<tbody>
<tr>
<td>DBNet</td>
<td>CRNN</td>
<td>ICDAR15</td>
<td>57.82</td>
<td>4.86</td>
</tr>
<tr>
<td>PSENet</td>
<td>CRNN</td>
<td>ICDAR15</td>
<td>47.91</td>
<td>1.65</td>
</tr>
<tr>
<td>PSENet (det_limit_side_len=1472 )</td>
<td>CRNN</td>
<td>ICDAR15</td>
<td>55.51</td>
<td>0.44</td>
</tr>
<tr>
<td>DBNet++</td>
<td>RARE</td>
<td>ICDAR15</td>
<td>59.17</td>
<td>3.47</td>
</tr>
<tr>
<td>DBNet++</td>
<td>SVTR</td>
<td>ICDAR15</td>
<td>64.42</td>
<td>2.49</td>
</tr>
</tbody>
</table>
<p></center></p>
<p><strong>Notes:</strong>
1. Currently, online inference pipeline is not optimized for efficiency, thus FPS is only for comparison between models. If FPS is your highest priority, please refer to <a href="https://github.com/mindspore-lab/mindocr/blob/main/docs/en/inference/inference_tutorial.md">Inference on Ascend 310</a>, which is much faster.
2. Unless extra inidication, all experiments are run with <code>--det_limit_type</code>="min" and <code>--det_limit_side</code>=720.
3. SVTR is run in mixed precision mode (amp_level=O2) since it is optimized for O2.</p>
<h3 id="text-direction-classification">Text direction classification<a class="headerlink" href="#text-direction-classification" title="Permanent link">&para;</a></h3>
<p>If there are non-upright text characters in the image, they can be classified and corrected for orientation using a text direction classifier after the detection. If you run text direction classification and correction on an input image, please perform
<div class="highlight"><pre><span></span><code>python<span class="w"> </span>tools/infer/text/predict_system.py<span class="w"> </span>--image_dir<span class="w"> </span><span class="o">{</span>path_to_img<span class="w"> </span>or<span class="w"> </span>dir_to_imgs<span class="o">}</span><span class="w"> </span><span class="se">\</span>
<span class="w">                                          </span>--det_algorithm<span class="w"> </span>DB++<span class="w">  </span><span class="se">\</span>
<span class="w">                                          </span>--rec_algorithm<span class="w"> </span>CRNN<span class="w">  </span><span class="se">\</span>
<span class="w">                                          </span>--cls_algorithm<span class="w"> </span>M3
</code></pre></div>
The default parameter <code>--cls_alorithm</code> is None, which means that text direction classification is not performed. By setting <code>--cls_alorithm</code>, text direction classification is performed in the text detection and recognition flow. In the process of execution, the text direction classifier classifies the list of images detected by the text and corrects the direction of the non-upright images. Here are some examples of the results.</p>
<ul>
<li>Text direction classification</li>
</ul>
<p align="center">
  <img src="https://raw.githubusercontent.com/zhangjunlongtech/Material/refs/heads/main/CRNN_t1.png" width=150 />
</p>
<p align="center">
  <em> word_01.png </em>
</p>

<p align="center">
  <img src="https://raw.githubusercontent.com/zhangjunlongtech/Material/refs/heads/main/CRNN_t2.png" width=150 />
</p>
<p align="center">
  <em> word_02.png </em>
</p>

<p>Classification Results:：
<div class="highlight"><pre><span></span><code>word_01.png   0     1.0
word_02.png   180   1.0
</code></pre></div></p>
<p>The currently supported text direction classification network is <code>mobilnet_v3</code>, which can be set by configuring <code>--cls_algorithm</code> for <code>M3</code>. And through <code>--cls_amp_level</code> and <code>--cls_model_dir</code> to set the text direction classifier automatic mixing precision and weight file. At present, the default weight file has been configured, the default mixing precision of the network is <code>O0</code>, and the direction classification supports <code>0</code> and <code>180</code> degrees under the default configuration. We will support the classification of other directions in the future.</p>
<p><center></p>
<table>
<thead>
<tr>
<th style="text-align: center;"><strong>Algorithm Name</strong></th>
<th style="text-align: center;"><strong>Network Name</strong></th>
<th style="text-align: center;"><strong>Language</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">M3</td>
<td style="text-align: center;">mobilenet_v3</td>
<td style="text-align: center;">CH/EN</td>
</tr>
</tbody>
</table>
<p></center></p>
<p>In addition, by setting <code>--save_cls_result</code> to <code>True</code>, text orientation classification results can be saved to <code>{args.crop_res_save_dir}/cls_results.txt</code>, Where <code>--crop_res_save_dir</code> is the directory where the results are saved.</p>
<p>For more parameter descriptions and usage information, please refer to <code>tools/infer/text/config.py</code>.</p>
<h2 id="table-structure-recognition">Table Structure Recognition<a class="headerlink" href="#table-structure-recognition" title="Permanent link">&para;</a></h2>
<p>To run table structure recognition on an input image or multiple images in a directory, please run:</p>
<div class="highlight"><pre><span></span><code>python<span class="w"> </span>tools/infer/text/predict_table_structure.py<span class="w"> </span>--image_dir<span class="w"> </span><span class="o">{</span>path_to_img<span class="w"> </span>or<span class="w"> </span>dir_to_imgs<span class="o">}</span><span class="w"> </span>--table_algorithm<span class="w"> </span>TABLE_MASTER
</code></pre></div>
<p>After running, the inference results will be saved in <code>{args.draw_img_save_dir}</code>, where <code>--draw_img_save_dir</code> is the directory for saving  results and is set to <code>./inference_results</code> by default. Here are some results for examples.</p>
<p>Example 1：</p>
<p>The sample image is <code>configs/table/example.png</code>. The inference result is as follows:</p>
<p align="center">
  <img src="https://github.com/user-attachments/assets/753588ff-3c24-4bf9-95c5-61c4c87d03f3" width=1000 />
</p>
<p align="center">
  <em> example_structure.png </em>
</p>

<p><strong>Notes:</strong>
1. For more argument illustrations and usage, please run <code>python tools/infer/text/predict_table_structure.py -h</code> or view <code>tools/infer/text/config.py</code></p>
<h3 id="supported-table-structure-recognition-algorithms-and-networks">Supported Table Structure Recognition Algorithms and Networks<a class="headerlink" href="#supported-table-structure-recognition-algorithms-and-networks" title="Permanent link">&para;</a></h3>
<p><center></p>
<table>
<thead>
<tr>
<th style="text-align: center;"><strong>Model Name</strong></th>
<th style="text-align: center;"><strong>Backbone</strong></th>
<th style="text-align: center;"><strong>Language</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">table_master</td>
<td style="text-align: center;">table_resnet_extra</td>
<td style="text-align: center;">universal</td>
</tr>
</tbody>
</table>
<p></center></p>
<p>The algorithm-network mapping is defined in <code>tools/infer/text/predict_table_structure.py</code>.</p>
<h2 id="table-structure-recognition-and-text-detection-recognition-concatenation">Table Structure Recognition and Text Detection Recognition Concatenation<a class="headerlink" href="#table-structure-recognition-and-text-detection-recognition-concatenation" title="Permanent link">&para;</a></h2>
<p>To run table recognition on an input image or multiple images in a directory (i.e., recognize the table structure first, then combine the results of text detection and recognition to recognize the complete table content), and recovery to CSV files, please run:
<div class="highlight"><pre><span></span><code>python<span class="w"> </span>tools/infer/text/predict_table_recognition.py<span class="w"> </span>--image_dir<span class="w"> </span><span class="o">{</span>path_to_img<span class="w"> </span>or<span class="w"> </span>dir_to_imgs<span class="o">}</span><span class="w"> </span><span class="se">\</span>
<span class="w">                                          </span>--det_algorithm<span class="w"> </span>DB_PPOCRv3<span class="w">  </span><span class="se">\</span>
<span class="w">                                          </span>--rec_algorithm<span class="w"> </span>SVTR_PPOCRv3_CH<span class="w"> </span><span class="se">\</span>
<span class="w">                                          </span>--table_algorithm<span class="w"> </span>TABLE_MASTER
</code></pre></div></p>
<p>After running, the inference results will be saved in <code>{args.draw_img_save_dir}</code>, where <code>--draw_img_save_dir</code> is the directory for saving  results and is set to <code>./inference_results</code> by default. Here are some results for examples.</p>
<p>Example 1：</p>
<p>The sample image is <code>configs/table/example.png</code>. After online inference, the content of the CSV file is as follows:</p>
<div class="highlight"><pre><span></span><code>Parameter,Non-smokers Mean± SD or N (3),Smokers Mean ± SD or N (C)
N,24,
Age (y),69.1 ± 7.0,61.5 ± 9.3 +
Males/Females,24/0,11/0
Race White/Black,19/5,9/2
Weight (kg),97.8 ± 16.8,102.5 ± 23.4
BMII (kg/m*),32.6 ± 4.9,32.6 ± 6.6
Serum albumin (g/dL),3.8 ± 0.33,3.63 ± 0.30
Serum Creatinine (mg/dL),2.75 ± 1.21,1.80 ± 0.74 *
BUN (mg/dL),46.5 ± 25.6,38.3 ± 21.8
Hemoglobin (g/dL),13.3 ± 1.6,13.5 ± 2.4
24 hour urine protein (g/d),3393 ± 2522,4423 ± 4385
lathae)mm,28.9 ± 13.8,47.2 ± 34.8 *
Duration of diabetes (yr),15.7 ± 9.1,13.3 ± 9.0
Insulin use,15 (63%),6 (55%)
&quot;Hemoglobin A, C (%)&quot;,7.57 ± 2.02,8.98 ± 2.93
Waist/Hip Ratio,1.00 ± 0.07,1.04 ± 0.07
Antihypertensive medications,4.3 ± 1.6,3.9 ± 1.9
A,21 (88%),8 (73%)
Total Cholesterol (mg/dL),184 ± 51,223 ± 87
LDL Cholesterol (mg/dL),100 ± 44,116 ± 24
HDL Cholesterol (mg/dL),42 ± 11.1,46 ± 11.4
,17 (71%),7 (64%)
</code></pre></div>
<p><strong>Notes:</strong>
1. For more argument illustrations and usage, please run <code>python tools/infer/text/predict_table_recognition.py -h</code> or view <code>tools/infer/text/config.py</code></p>
<h2 id="layout-analysis">Layout Analysis<a class="headerlink" href="#layout-analysis" title="Permanent link">&para;</a></h2>
<p>To run layout analysis on an input image or a directory containing multiple images, please execute
<div class="highlight"><pre><span></span><code>python<span class="w"> </span>tools/infer/text/predict_layout.py<span class="w">  </span>--image_dir<span class="w"> </span><span class="o">{</span>path_to_img<span class="w"> </span>or<span class="w"> </span>dir_to_imgs<span class="o">}</span><span class="w"> </span>--layout_algorithm<span class="w"> </span>YOLOv8<span class="w"> </span>--visualize_output<span class="w"> </span>True
</code></pre></div>
After running, the inference results will be saved in <code>{args.draw_img_save_dir}/det_results.txt</code>, where <code>--draw_img_save_dir</code> is the directory for saving  results and is set to <code>./inference_results</code> by default Here are some results for examples.</p>
<p>Example 1:</p>
<p align="center">
  <img src="https://github.com/user-attachments/assets/0cc501a8-5764-4b3a-8080-3dbc0f3ecb5e" width=480>
</p>
<p align="center">
  <em> Visualization of layout analysis result on PMC4958442_00003.jpg</em>
</p>

<p>, where the saved layout_result.txt file is as follows
<div class="highlight"><pre><span></span><code>{&quot;image_id&quot;: 0, &quot;category_id&quot;: 1, &quot;bbox&quot;: [308.649, 559.189, 240.211, 81.412], &quot;score&quot;: 0.98431}
{&quot;image_id&quot;: 0, &quot;category_id&quot;: 1, &quot;bbox&quot;: [50.435, 673.018, 240.232, 70.262], &quot;score&quot;: 0.98414}
{&quot;image_id&quot;: 0, &quot;category_id&quot;: 3, &quot;bbox&quot;: [322.805, 348.831, 225.949, 203.302], &quot;score&quot;: 0.98019}
{&quot;image_id&quot;: 0, &quot;category_id&quot;: 1, &quot;bbox&quot;: [308.658, 638.657, 240.31, 70.583], &quot;score&quot;: 0.97986}
{&quot;image_id&quot;: 0, &quot;category_id&quot;: 1, &quot;bbox&quot;: [50.616, 604.736, 240.044, 70.086], &quot;score&quot;: 0.9797}
{&quot;image_id&quot;: 0, &quot;category_id&quot;: 1, &quot;bbox&quot;: [50.409, 423.237, 240.132, 183.652], &quot;score&quot;: 0.97805}
{&quot;image_id&quot;: 0, &quot;category_id&quot;: 1, &quot;bbox&quot;: [308.66, 293.918, 240.181, 47.497], &quot;score&quot;: 0.97471}
{&quot;image_id&quot;: 0, &quot;category_id&quot;: 1, &quot;bbox&quot;: [308.64, 707.13, 240.271, 36.028], &quot;score&quot;: 0.97427}
{&quot;image_id&quot;: 0, &quot;category_id&quot;: 1, &quot;bbox&quot;: [308.697, 230.568, 240.062, 43.545], &quot;score&quot;: 0.96921}
{&quot;image_id&quot;: 0, &quot;category_id&quot;: 4, &quot;bbox&quot;: [51.787, 100.444, 240.267, 273.653], &quot;score&quot;: 0.96839}
{&quot;image_id&quot;: 0, &quot;category_id&quot;: 5, &quot;bbox&quot;: [308.637, 74.439, 237.878, 149.174], &quot;score&quot;: 0.96707}
{&quot;image_id&quot;: 0, &quot;category_id&quot;: 1, &quot;bbox&quot;: [50.615, 70.667, 240.068, 22.0], &quot;score&quot;: 0.94156}
{&quot;image_id&quot;: 0, &quot;category_id&quot;: 2, &quot;bbox&quot;: [50.549, 403.5, 67.392, 12.85], &quot;score&quot;: 0.92577}
{&quot;image_id&quot;: 0, &quot;category_id&quot;: 1, &quot;bbox&quot;: [51.384, 374.84, 171.939, 10.736], &quot;score&quot;: 0.76692}
</code></pre></div>
In this file, <code>image_id</code> is the image ID, <code>bbox</code> is the detected bounding box <code>[x-coordinate of the top-left corner, y-coordinate of the bottom-right corner, width, height]</code>, <code>score</code> is the detection confidence, and <code>category_id</code> has the following meanings:
- <code>1: text</code>
- <code>2: title</code>
- <code>3: list</code>
- <code>4: table</code>
- <code>5: figure</code></p>
<p><strong>Notes:</strong>
- For more argument illustrations and usage, please run <code>python tools/infer/text/predict_layout.py -h</code> or view <code>tools/infer/text/config.py</code></p>
<h2 id="end-to-end-document-analysis-and-recovery">End-to-end Document Analysis and Recovery<a class="headerlink" href="#end-to-end-document-analysis-and-recovery" title="Permanent link">&para;</a></h2>
<p>To run end-to-end document analysis and recovery on an input image or multiple images in a directory (detecting all the text, table, and figure regions, recognizing words in these regions, and putting everything into docx files according to the original layout), please run:</p>
<div class="highlight"><pre><span></span><code>python<span class="w"> </span>tools/infer/text/predict_table_e2e.py<span class="w"> </span>--image_dir<span class="w"> </span><span class="o">{</span>path_to_img<span class="w"> </span>or<span class="w"> </span>dir_to_imgs<span class="o">}</span><span class="w"> </span><span class="se">\</span>
<span class="w">                                             </span>--det_algorithm<span class="w"> </span><span class="o">{</span>DET_ALGO<span class="o">}</span><span class="w"> </span><span class="se">\</span>
<span class="w">                                             </span>--rec_algorithm<span class="w"> </span><span class="o">{</span>REC_ALGO<span class="o">}</span>
</code></pre></div>
<blockquote>
<p>Note: To visualize the outputs of layout, table and ocr, please set <code>--visualize_output True</code>.</p>
</blockquote>
<p>After running, the inference results will be saved in <code>{args.draw_img_save_dir}/{img_name}_e2e_result.txt</code>, where <code>--draw_img_save_dir</code> is the directory for saving  results and is set to <code>./inference_results</code> by default. Here are some results for examples.</p>
<p>Example 1:</p>
<p align="center">
  <img src="https://github.com/user-attachments/assets/f1578fda-c5c7-46ba-b446-d0dc65edf2d7"/>
</p>
<p align="center">
  <em> PMC4958442_00003.jpg Converting into docx </em>
</p>

<p>, where the saved txt file is as follows
<div class="highlight"><pre><span></span><code>{&quot;type&quot;: &quot;text&quot;, &quot;bbox&quot;: [50.615, 70.667, 290.683, 92.667], &quot;res&quot;: &quot;tabley predictive value ofbasic clinical laboratory and suciode variables surney anc yea after tramphenins&quot;, &quot;layout&quot;: &quot;double&quot;}
{&quot;type&quot;: &quot;table&quot;, &quot;bbox&quot;: [51.787, 100.444, 292.054, 374.09700000000004], &quot;res&quot;: &quot;&lt;html&gt;&lt;body&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;td&gt;&lt;b&gt;sign factor&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;prediction valucofthe the&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;from difereness significance levelaf the&lt;/b&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;gender&lt;/td&gt;&lt;td&gt;0027 0021&lt;/td&gt;&lt;td&gt;o442&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;00z44&lt;/td&gt;&lt;td&gt;0480&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;cause&lt;/td&gt;&lt;td&gt;tooza 0017&lt;/td&gt;&lt;td&gt;o547&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;cadaverieilizing donorst&lt;/td&gt;&lt;td&gt;0013 aont&lt;/td&gt;&lt;td&gt;0740&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;induction transplantation before dialysis&lt;/td&gt;&lt;td&gt;doattoos&lt;/td&gt;&lt;td&gt;0125&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;depleting antibodies monoclomalor cn immunosuppression with&lt;/td&gt;&lt;td&gt;doista09&lt;/td&gt;&lt;td&gt;0230&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;ititis&lt;/td&gt;&lt;td&gt;0029&lt;/td&gt;&lt;td&gt;aaso&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;status itional&lt;/td&gt;&lt;td&gt;0047 toots&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;townfrillage&lt;/td&gt;&lt;td&gt;non&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;transplantations number&lt;/td&gt;&lt;td&gt;toos 0017&lt;/td&gt;&lt;td&gt;o5s1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;creatinine&lt;/td&gt;&lt;td&gt;02400g&lt;/td&gt;&lt;td&gt;caoor&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;pressure bload systolic&lt;/td&gt;&lt;td&gt;aidaloloss&lt;/td&gt;&lt;td&gt;aoz&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;pressure diastolic blood&lt;/td&gt;&lt;td&gt;dobetods&lt;/td&gt;&lt;td&gt;ass&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;hemoglobin&lt;/td&gt;&lt;td&gt;0044 0255t&lt;/td&gt;&lt;td&gt;caoor&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;004&lt;/td&gt;&lt;td&gt;caoor&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/body&gt;&lt;/html&gt;&quot;, &quot;layout&quot;: &quot;double&quot;}
{&quot;type&quot;: &quot;text&quot;, &quot;bbox&quot;: [51.384, 374.84, 223.32299999999998, 385.57599999999996], &quot;res&quot;: &quot;nanc rmeans more significant forecasting factor sign&quot;, &quot;layout&quot;: &quot;double&quot;}
{&quot;type&quot;: &quot;title&quot;, &quot;bbox&quot;: [50.549, 403.5, 117.941, 416.35], &quot;res&quot;: &quot;discussion&quot;, &quot;layout&quot;: &quot;double&quot;}
{&quot;type&quot;: &quot;text&quot;, &quot;bbox&quot;: [50.409, 423.237, 290.541, 606.889], &quot;res&quot;: &quot;determination of creatinine and hemoglobin level in the blood well aetho concentration of protein in the urine in one year atter kidney transplantation with the calculation of prognostic criterion predics the loss of renal allotransplant function in years fafter surgery advantages ff the method are the possibility oof quantitative forecasting of renal allotransplant losser which based not only its excretory function assessment but also on assessment other characteristics that may have important prognostic value and does not always directly correlate with changes in its excretors function in order the riskof death with transplant sfunctioning returntothe program hemodialysis the predictive model was implemented cabular processor excel forthe useofthe model litisquite enough the value ethel given indices calculation and prognosis will be automatically done in the electronic table figure 31&quot;, &quot;layout&quot;: &quot;double&quot;}
{&quot;type&quot;: &quot;text&quot;, &quot;bbox&quot;: [50.616, 604.736, 290.66, 674.822], &quot;res&quot;: &quot;the calculator designed by us has been patented chttpell napatentscomy 68339 sposib prognozuvannys vtrati funk caniskovogo transplanatchti and disnvailable on the in ternet chitpsolivad skillwond the accuract ot prediction of renal transplant function loss three years after transplantation was 92x&quot;, &quot;layout&quot;: &quot;double&quot;}
{&quot;type&quot;: &quot;text&quot;, &quot;bbox&quot;: [50.435, 673.018, 290.66700000000003, 743.28], &quot;res&quot;: &quot;progression of chronic renal dysfunctional the transplant accompanied the simultaneous losa the benefits of successful transplantation and the growth of problems due to immunosuppresson bosed on retrospective analysis nt resultsof treatment tofkidney transplantof the recipients with blood creatinine higher than d3 immold we adhere to the&quot;, &quot;layout&quot;: &quot;double&quot;}
{&quot;type&quot;: &quot;figure&quot;, &quot;bbox&quot;: [308.637, 74.439, 546.515, 223.613], &quot;res&quot;: &quot;./inference_results/example_figure_10.png&quot;, &quot;layout&quot;: &quot;double&quot;}
{&quot;type&quot;: &quot;text&quot;, &quot;bbox&quot;: [308.697, 230.568, 548.759, 274.113], &quot;res&quot;: &quot;figures the cnerhecadfmuthrnatical modeltor prognostication ofkidaey transplant function during the periodal three years after thetransplantation according oletectercipiolgaps after theoperation&quot;, &quot;layout&quot;: &quot;double&quot;}
{&quot;type&quot;: &quot;text&quot;, &quot;bbox&quot;: [308.66, 293.918, 548.841, 341.415], &quot;res&quot;: &quot;following principles in thecorrectionod immunisuppresion which allow decreasing the rateofs chronic dysfunctionof the transplant development or edecreasing the risk fof compliea tions incaeoflasof function&quot;, &quot;layout&quot;: &quot;double&quot;}
{&quot;type&quot;: &quot;list&quot;, &quot;bbox&quot;: [322.805, 348.831, 548.754, 552.133], &quot;res&quot;: &quot;wdo not prescribe hish doses steroids and do have the steroid pulse therapy cy do not increase the dose of received cyclosporine tacrolimus and stop medication ifthere isan increase in nephropathy tj continue immunosuppression with medicines ofmy cophenolic acid which are not nephrotoxic k4 enhance amonitoring of immunosuppression andpe vention infectious com cancel immunosuppression atreturning hemodi alysis treatment cancellation of steroids should done egradually sometimes for several months when thediscomfort eassociated transplant tempera ture main in the projection the transplanted kidney and hematurial short course of low doses of steroids administered orally of intravenously can be effective&quot;, &quot;layout&quot;: &quot;double&quot;}
{&quot;type&quot;: &quot;text&quot;, &quot;bbox&quot;: [308.649, 559.189, 548.86, 640.601], &quot;res&quot;: &quot;according to plasma concentration of creatinine the return hemodialvsis the patients were divided into groups ln the first group the creatinine concentration in blood plasma waso mmoly in the 2nd groun con centration in blood plasma was azlommaty and in the third group concentration in blood plasma was more than commolt&quot;, &quot;layout&quot;: &quot;double&quot;}
{&quot;type&quot;: &quot;text&quot;, &quot;bbox&quot;: [308.658, 638.657, 548.9680000000001, 709.24], &quot;res&quot;: &quot;dates or the return of transplant recipients with delaved rena transplant disfunction are largely dependent ion the psychological state ofthe patient severity of depression the desire to ensure the irreversibility the transplanted kidney dysfunction and fear that the dialysis will contribute to the deterioration of renal transplant function&quot;, &quot;layout&quot;: &quot;double&quot;}
{&quot;type&quot;: &quot;text&quot;, &quot;bbox&quot;: [308.64, 707.13, 548.911, 743.158], &quot;res&quot;: &quot;the survival rateof patients ofthe first group after return in hemodialysis was years and in the second and third groups respectively 53132 and28426 years&quot;, &quot;layout&quot;: &quot;double&quot;}
</code></pre></div>
In this file, <code>type</code> is the classification of the detected region, <code>bbox</code> is the detected bounding box <code>[x-coordinate of the top-left corner, y-coordinate of the bottom-right corner, width, height]</code>, and <code>res</code> is the detected result.</p>
<p><strong>Notes:</strong>
1. For more argument illustrations and usage, please run <code>python tools/infer/text/predict_table_e2e.py -h</code> or view <code>tools/infer/text/config.py</code>
2. Besides the parameters in the config.py, predict_table_e2e.py also accepts the following parameters:
<center></p>
<table>
<thead>
<tr>
<th style="text-align: center;"><strong>Parameter</strong></th>
<th style="text-align: center;"><strong>Description</strong></th>
<th style="text-align: center;"><strong>Default</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">layout</td>
<td style="text-align: center;">Layout Analysis</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">ocr</td>
<td style="text-align: center;">Text Recognition</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">table</td>
<td style="text-align: center;">Table Analysis</td>
<td style="text-align: center;">True</td>
</tr>
<tr>
<td style="text-align: center;">recovery</td>
<td style="text-align: center;">Docx Convertion</td>
<td style="text-align: center;">True</td>
</tr>
</tbody>
</table>
<p></center></p>
<h2 id="argument-list">Argument List<a class="headerlink" href="#argument-list" title="Permanent link">&para;</a></h2>
<p>All CLI argument definition can be viewed via <code>python tools/infer/text/predict_system.py -h</code> or reading <code>tools/infer/text/config.py</code>.</p>
<h2 id="developer-guide-how-to-add-a-new-model-for-inference">Developer Guide - How to Add a New Model for Inference<a class="headerlink" href="#developer-guide-how-to-add-a-new-model-for-inference" title="Permanent link">&para;</a></h2>
<h3 id="preprocessing">Preprocessing<a class="headerlink" href="#preprocessing" title="Permanent link">&para;</a></h3>
<p>The optimal preprocessing strategy can vary from model to model, especially for the resize setting (keep_ratio, padding, etc). We define the preprocessing pipeline for each model in <code>tools/infer/text/preprocess.py</code> for different tasks.</p>
<p>If you find the default preprocessing pipeline or hyper-params does not meet the network requirement, please extend by changing the if-else conditions or adding a new key-value pair to the <code>optimal_hparam</code> dict in <code>tools/infer/text/preprocess.py</code>, where key is the algorithm name and the value is the suitable hyper-param setting for the target network inference.</p>
<h3 id="network-inference">Network Inference<a class="headerlink" href="#network-inference" title="Permanent link">&para;</a></h3>
<p>Supported alogirhtms and their corresponding network names (which can be checked by using the <code>list_model()</code> API) are defined in the <code>algo_to_model_name</code> dict in <code>predict_det.py</code> and <code>predict_rec.py</code>.</p>
<p>To add a new detection model for inference, please add a new key-value pair to <code>algo_to_model_name</code> dict, where the key is an algorithm name and the value is the corresponding network name registered in <code>mindocr/models/{your_model}.py</code>.</p>
<p>By default, model weights will be loaded from the pro-defined URL in <code>mindocr/models/{your_model}.py</code>. If you want to load a local checkpoint instead, please set <code>--det_model_dir</code> or <code>--rec_model_dir</code> to the path of your local checkpoint or the directory containing a model checkpoint.</p>
<h3 id="postproprocess">Postproprocess<a class="headerlink" href="#postproprocess" title="Permanent link">&para;</a></h3>
<p>Similar to preprocessing, the postprocessing method for each algorithm can vary. The postprocessing method for each algorithm is defined in <code>tools/infer/text/postprocess.py</code>.</p>
<p>If you find the default postprocessing method or hyper-params does not meet the model need, please extend the if-else conditions or add a new key-value pair  to the <code>optimal_hparam</code> dict in <code>tools/infer/text/postprocess.py</code>, where the key is an algorithm name and the value is the hyper-param setting.</p>
<!-- END INCLUDE -->












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../../tutorials/advanced_train/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Advance Training">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Advance Training
              </div>
            </div>
          </a>
        
        
          
          <a href="../../inference/inference_tutorial/" class="md-footer__link md-footer__link--next" aria-label="Next: MindOCR Offline Inference">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                MindOCR Offline Inference
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2022 - 2025 MindSpore Lab
    </div>
  
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="mailto:mindspore-lab@huawei.com" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M498.1 5.6c10.1 7 15.4 19.1 13.5 31.2l-64 416c-1.5 9.7-7.4 18.2-16 23s-18.9 5.4-28 1.6L284 427.7l-68.5 74.1c-8.9 9.7-22.9 12.9-35.2 8.1S160 493.2 160 480v-83.6c0-4 1.5-7.8 4.2-10.8l167.6-182.8c5.8-6.3 5.6-16-.4-22s-15.7-6.4-22-.7L106 360.8l-88.3-44.2C7.1 311.3.3 300.7 0 288.9s5.9-22.8 16.1-28.7l448-256c10.7-6.1 23.9-5.5 34 1.4"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/mindspore-lab/mindocr" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.zhihu.com/people/mindsporelab" target="_blank" rel="noopener" title="www.zhihu.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M170.54 148.13v217.54l23.43.01 7.71 26.37 42.01-26.37h49.53V148.13zm97.75 193.93h-27.94l-27.9 17.51-5.08-17.47-11.9-.04V171.75h72.82zm-118.46-94.39H97.5c1.74-27.1 2.2-51.59 2.2-73.46h51.16s1.97-22.56-8.58-22.31h-88.5c3.49-13.12 7.87-26.66 13.12-40.67 0 0-24.07 0-32.27 21.57-3.39 8.9-13.21 43.14-30.7 78.12 5.89-.64 25.37-1.18 36.84-22.21 2.11-5.89 2.51-6.66 5.14-14.53h28.87c0 10.5-1.2 66.88-1.68 73.44H20.83c-11.74 0-15.56 23.62-15.56 23.62h65.58C66.45 321.1 42.83 363.12 0 396.34c20.49 5.85 40.91-.93 51-9.9 0 0 22.98-20.9 35.59-69.25l53.96 64.94s7.91-26.89-1.24-39.99c-7.58-8.92-28.06-33.06-36.79-41.81L87.9 311.95c4.36-13.98 6.99-27.55 7.87-40.67h61.65s-.09-23.62-7.59-23.62zm412.02-1.6c20.83-25.64 44.98-58.57 44.98-58.57s-18.65-14.8-27.38-4.06c-6 8.15-36.83 48.2-36.83 48.2zm-150.09-59.09c-9.01-8.25-25.91 2.13-25.91 2.13s39.52 55.04 41.12 57.45l19.46-13.73s-25.67-37.61-34.66-45.86h-.01zM640 258.35c-19.78 0-130.91.93-131.06.93v-101c4.81 0 12.42-.4 22.85-1.2 40.88-2.41 70.13-4 87.77-4.81 0 0 12.22-27.19-.59-33.44-3.07-1.18-23.17 4.58-23.17 4.58s-165.22 16.49-232.36 18.05c1.6 8.82 7.62 17.08 15.78 19.55 13.31 3.48 22.69 1.7 49.15.89 24.83-1.6 43.68-2.43 56.51-2.43v99.81H351.41s2.82 22.31 25.51 22.85h107.94v70.92c0 13.97-11.19 21.99-24.48 21.12-14.08.11-26.08-1.15-41.69-1.81 1.99 3.97 6.33 14.39 19.31 21.84 9.88 4.81 16.17 6.57 26.02 6.57 29.56 0 45.67-17.28 44.89-45.31v-73.32h122.36c9.68 0 8.7-23.78 8.7-23.78z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tracking", "navigation.tabs", "navigation.sections", "navigation.indexes", "navigation.top", "navigation.footer", "toc.follow", "search.highlight", "search.share", "search.suggest", "content.action.view", "content.action.edit", "content.tabs.link", "content.code.copy", "content.code.select", "content.code.annotations"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.c8b220af.min.js"></script>
      
    
  </body>
</html>