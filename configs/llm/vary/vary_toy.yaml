model:
  name: VaryQwenForCausalLM
  batch_size: 1
  seq_length: 2048
  hidden_size: 2048
  num_layers: 24
  num_heads: 16
  vocab_size: 151860
  intermediate_size: 5504
  rms_norm_eps: 1.0e-6
  emb_dropout_prob: 0.0
  eos_token_id: 151643
  pad_token_id: 151643
  compute_dtype: "float16"
  layernorm_compute_type: "float32"
  softmax_compute_type: "float16"
  rotary_dtype: "float16"
  param_init_type: "float16"
  ln_param_init_type: "float16"
  use_past: True
  use_flash_attention: False
  use_past_shard: False
  offset: 0
  checkpoint_name_or_path: "/path/to/vary_toy.ckpt"
  repetition_penalty: 1.5
  max_decode_length: 2048
  top_k: 0
  top_p: 0.8
  do_sample: False
  max_new_tokens: 1024
  temperature: 1.0
  num_beams: 1

  # configuration items copied from Qwen
  rotary_pct: 1.0
  rotary_emb_base: 10000
  kv_channels: 128

processor:
  return_tensors: ms
  tokenizer:
    vocab_file: "/path/to/qwen.tiktoken"
    pad_token: "<|endoftext|>"
  type: QwenProcessor
